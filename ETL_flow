import os
from airflow import DAG
from datetime import timedelta, datetime, timezone
from airflow.operators.empty import EmptyOperator
from airflow.providers.http.sensors.http import HttpSensor
import json
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
import pandas as pd
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.utils.task_group import TaskGroup
from airflow.models import Variable

def kelvin_to_fahrenheit(temp_in_kelvin):
    return (temp_in_kelvin - 273.15) * 9 / 5 + 32

def transform_load_data(task_instance):
    data = task_instance.xcom_pull(task_ids="extract_weather_data")
    if data is None:
        raise ValueError("No data retrieved from 'extract_weather_data'. Please check the previous task.")
    
    city = data["name"]
    weather_description = data["weather"][0]['description']
    temp_fahrenheit = kelvin_to_fahrenheit(data["main"]["temp"])
    feels_like_fahrenheit = kelvin_to_fahrenheit(data["main"]["feels_like"])
    min_temp_fahrenheit = kelvin_to_fahrenheit(data["main"]["temp_min"])
    max_temp_fahrenheit = kelvin_to_fahrenheit(data["main"]["temp_max"])
    pressure = data["main"]["pressure"]
    humidity = data["main"]["humidity"]
    wind_speed = data["wind"]["speed"]
    time_of_record = datetime.fromtimestamp(data['dt'] + data['timezone'], tz=timezone.utc)
    sunrise_time = datetime.fromtimestamp(data['sys']['sunrise'] + data['timezone'], tz=timezone.utc)
    sunset_time = datetime.fromtimestamp(data['sys']['sunset'] + data['timezone'], tz=timezone.utc)

    transformed_data = {
        "City": city,
        "Description": weather_description,
        "Temperature (F)": temp_fahrenheit,
        "Feels Like (F)": feels_like_fahrenheit,
        "Minimum Temp (F)": min_temp_fahrenheit,
        "Maximum Temp (F)": max_temp_fahrenheit,
        "Pressure": pressure,
        "Humidity": humidity,
        "Wind Speed": wind_speed,
        "Time of Record": time_of_record,
        "Sunrise (Local Time)": sunrise_time,
        "Sunset (Local Time)": sunset_time
    }
    
    df_data = pd.DataFrame([transformed_data])
    dt_string = datetime.now().strftime("%Y%m%d")
    file_path = f"/tmp/current_weather_data_korea_{dt_string}.csv"
    df_data.to_csv(file_path, index=False)
    task_instance.xcom_push(key='file_path', value=file_path)

def log_response_data(task_instance):
    # XCom에서 데이터 가져오기
    dt = task_instance.xcom_pull(task_ids='extract_weather_data')
    
    # 데이터 로깅
    if dt is not None:
        print("Received data:", dt)  # 콘솔에 출력 (주로 로컬 테스트에 유용)
        logging.info(f"Received data: {dt}")  # Airflow 로그에 기록
    else:
        logging.warning("No data received from 'extract_weather_data'")


def upload_to_s3(task_instance):
    file_path = task_instance.xcom_pull(key='file_path', task_ids='transform_load_weather_data')
    bucket_name = 'dataintelligence-s3bucket'
    s3_key = f"airflow_etl/weather_data/{os.path.basename(file_path)}"
    hook = S3Hook('aws_airflow_conn')
    hook.load_file(filename=file_path, key=s3_key, bucket_name=bucket_name)

def download_from_s3(task_instance, bucket_name, s3_key, local_dir_path, aws_conn_id='aws_airflow_conn'):
    s3_key = f'airflow_etl/weather_data/current_weather_data_korea_{dt_string}.csv'
    local_file_path = os.path.join(local_dir_path, f'{datetime.now().strftime("%Y%m")}.csv')
    hook = S3Hook(aws_conn_id=aws_conn_id)
    downloaded_file_name = hook.download_file(key=s3_key, bucket_name=bucket_name, local_path=local_file_path, preserve_file_name=True, use_autogenerated_subdir=False)
    return os.path.join(local_dir_path, downloaded_file_name)

def save_data_to_postgres():
    dt_string = datetime.now().strftime("%Y%m%d")
    file_name = f"current_weather_data_korea_{dt_string}.csv"
    csv_file_path = Variable.get("csv_file_path", default_var="/data/weather_data")
    full_file_path = os.path.join(csv_file_path, file_name)
    copy_sql = f"COPY weather_data FROM '{full_file_path}' WITH (FORMAT csv, HEADER);"

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 8),
    'email': ['julee@mz.co.kr'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2)
}

with DAG('weather_api_dag', default_args=default_args, schedule_interval='@daily', catchup=False) as dag:
    start_pipeline = EmptyOperator(task_id='tsk_start_pipeline')

    with TaskGroup(group_id='group_a', tooltip="Extract_from_S3_and_weatherapi") as group_A:
        #API 연결
        is_weather_api_ready = HttpSensor(
            task_id='is_weather_api_ready',
            http_conn_id='weathermap_api',
            endpoint='/data/2.5/weather?q=Korea&appid=fc2fa8d8b61fd0f4ed39548c2fa958c5'
            )

        extract_weather_data = SimpleHttpOperator(
            task_id='extract_weather_data',
            http_conn_id='weathermap_api',
            endpoint='/data/2.5/weather?q=Korea&appid=fc2fa8d8b61fd0f4ed39548c2fa958c5',
            method='GET',
            response_filter=lambda r: json.loads(r.text)
            )
        log_data_task = PythonOperator(
            task_id='log_data',
            python_callable=log_response_data,
            dag=dag
            )



        transform_load_weather_data = PythonOperator(
            task_id='transform_load_weather_data',
            python_callable=transform_load_data
            )

        upload_file_to_s3 = PythonOperator(
            task_id='upload_file_to_s3',
            python_callable=upload_to_s3
            )

        truncate_table = PostgresOperator(
            task_id='tsk_truncate_table',
            postgres_conn_id='postgres_conn',
            sql='TRUNCATE TABLE weather_data;'
            )

        download_task = PythonOperator(
            task_id='download_from_s3',
            python_callable=download_from_s3,
            op_kwargs={
                'bucket_name': 'dataintelligence-s3bucket',
                'local_dir_path': '/data/weather_data',
                'aws_conn_id': 'aws_airflow_conn'
            }
            )

        upload_data_to_postgres = PythonOperator(
            task_id='save_data_to_postgres',
            python_callable=save_data_to_postgres
            )

        is_weather_api_ready >> extract_weather_data >> transform_load_weather_data >> upload_file_to_s3 >> truncate_table >> download_task >> upload_data_to_postgres

    start_pipeline >> group_A
