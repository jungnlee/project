from __future__ import annotations

from airflow import DAG
#from airflow.operators.python_operator import PythonOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

import pendulum

import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

today = datetime.now().date()
yesterday = today - timedelta(days=1)
year_str = yesterday.strftime("%Y")
month_str = yesterday.strftime("%m")
day_str = yesterday.strftime("%d")
date_str = year_str + month_str + day_str
table_name = "building"
parquet_dir = '/home/ec2-user/parquet-csv/s3files/'
dag_name = "02.merge_parquet_building"
task_name = "merge_parquet_building"

def merge_parquet_files(input_dir, output_file):
    # parquet file list from input_dir
    parquet_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.parquet')]

    # create empty df
    merged_df = pd.DataFrame()

    # for
    for file in parquet_files:
        # read Parquet file
        table = pq.read_table(file)

        # transform df Parquet file
        df = table.to_pandas()

        # merge
        merged_df = pd.concat([merged_df, df], ignore_index=True)

    # save
    table_from_pandas = pa.Table.from_pandas(merged_df)
    pq.write_table(table_from_pandas, output_file)

    # remove parquet

with DAG(
    dag_id=dag_name,
    #schedule="0 2 * * *",
    schedule='@once',
    start_date=pendulum.datetime(2024, 3, 27, tz="UTC"),
    catchup=False
) as dag:

    input_directory = os.path.join(parquet_dir, table_name)
    output_file = os.path.join(parquet_dir, table_name, f"merged_{table_name}_{date_str}.parquet")

    merge_task = PythonOperator(
            task_id=task_name,
            python_callable=merge_parquet_files,
            op_kwargs={'input_dir': input_directory, 'output_file': output_file},
            dag=dag,
    )

    merge_task

