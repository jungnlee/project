from airflow import DAG
from datetime import timedelta, datetime
from airflow.operators.empty import EmptyOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.utils.task_group import TaskGroup

# DAG definition
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 8),
    'email': ['julee@mz.co.kr'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
}

with DAG('test_dag',
         default_args=default_args,
         schedule_interval='@daily',
         catchup=False) as dag:

    start_pipeline = EmptyOperator(
        task_id='tsk_start_pipeline'
    )

    with TaskGroup(group_id='group_a', tooltip="Extract_from_S3_and_weatherapi") as group_A:
        create_weather_data_table = PostgresOperator(
            task_id='tsk_create_weather_data_table',
            postgres_conn_id='postgres_conn',
            sql='''
            CREATE TABLE IF NOT EXISTS weather_data (
                city TEXT,
                description TEXT,
                temperature_f DOUBLE PRECISION,
                feels_like_f DOUBLE PRECISION,
                minimum_temp_f DOUBLE PRECISION,
                maximum_temp_f DOUBLE PRECISION,
                pressure INTEGER,
                humidity INTEGER,
                wind_speed DOUBLE PRECISION,
                time_of_record TIMESTAMP,
                sunrise_local_time TIMESTAMP,
                sunset_local_time TIMESTAMP
            );
            '''
        )

        truncate_table = PostgresOperator(
            task_id='tsk_truncate_table',
            postgres_conn_id='postgres_conn',
            sql='TRUNCATE TABLE weather_data;'
        )

        create_weather_data_table >> truncate_table

    start_pipeline >> group_A
